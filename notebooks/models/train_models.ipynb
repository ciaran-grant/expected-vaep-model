{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from expected_vaep_model.processing.data_preprocessor import ExpVAEPPreprocessor\n",
    "\n",
    "from AFLPy.AFLData_Client import load_data\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Chains and Expected Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chains = load_data(Dataset_Name='AFL_API_Match_Chains', ID = \"AFL\")\n",
    "xscore = load_data(Dataset_Name=\"CG_Expected_Score\", ID = \"AFL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chains.shape, xscore.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Expected Scores onto Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chains.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from expected_vaep_model.processing.merge_xscore_to_chains import merge_xscores_to_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xchains = merge_xscores_to_chains(chains, xscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xchains.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess into Expected VAEP Features / Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvaep_preproc = ExpVAEPPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_chains, gamestate_features, gamestate_labels = xvaep_preproc.transform(xchains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamestate_features[list(gamestate_features.select_dtypes(include='object'))] = gamestate_features.select_dtypes(include='object').astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scores, X_test_scores, y_train_scores, y_test_scores = train_test_split(\n",
    "    gamestate_features, \n",
    "    gamestate_labels['exp_scores_label'], \n",
    "    test_size=0.2, \n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_scores.mean(), y_test_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from expected_vaep_model.modelling.hyperparameter_tuning import XGBHyperparameterTuner\n",
    "from expected_vaep_model.modelling.optuna_xgb_param_grid import OptunaXGBParamGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tuner = XGBHyperparameterTuner(X_train_scores, y_train_scores)\n",
    "xgb_tuner.tune_hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from expected_vaep_model.modelling.supermodel import SuperXGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = xgb_tuner.get_best_params()\n",
    "params['objective'] = OptunaXGBParamGrid.error\n",
    "params['num_rounds'] = 100\n",
    "params['early_stopping_rounds'] = OptunaXGBParamGrid.early_stopping_rounds\n",
    "params['verbosity'] = OptunaXGBParamGrid.verbosity\n",
    "params['monotone_constraints'] = {}\n",
    "\n",
    "print('Fitting model.')\n",
    "super_xgb_scores = SuperXGBRegressor(X_train = X_train_scores, \n",
    "                            y_train = y_train_scores, \n",
    "                            X_test = X_test_scores, \n",
    "                            y_test = y_test_scores,\n",
    "                            params = params)\n",
    "super_xgb_scores.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score_preds = pd.Series(np.clip(super_xgb_scores.predict(X_train_scores), 0, 6), name = 'exp_scores')\n",
    "test_score_preds = pd.Series(np.clip(super_xgb_scores.predict(X_test_scores), 0, 6), name = 'exp_scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score_preds.index = X_train_scores.index\n",
    "test_score_preds.index = X_test_scores.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from expected_vaep_model.evaluation.model_evaluation import XGBRegressorEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evals_data = pd.concat([schema_chains.loc[X_train_scores.index], X_train_scores, y_train_scores, train_score_preds], axis=1)\n",
    "test_evals_data = pd.concat([schema_chains.loc[X_test_scores.index], X_test_scores, y_test_scores, test_score_preds], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evals_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evals_data[['mark_a1', 'mark_a2']] = test_evals_data[['mark_a1', 'mark_a2']].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test_evals = XGBRegressorEvaluator(model = super_xgb_scores.xgb_model,\n",
    "                                       data = test_evals_data,\n",
    "                                       actual_name = \"exp_scores_label\",\n",
    "                                       expected_name = \"exp_scores\"\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evals_data[['exp_scores_label', 'exp_scores']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test_evals.plot_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test_evals.plot_feature_importance(max_num_features=10, importance_type=\"total_gain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test_evals.plot_shap_summary_plot(sample=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concedes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_concedes, X_test_concedes, y_train_concedes, y_test_concedes = train_test_split(\n",
    "    gamestate_features, \n",
    "    gamestate_labels['exp_concedes_label'], \n",
    "    test_size=0.2, \n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_concedes.mean(), y_test_concedes.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from expected_vaep_model.modelling.hyperparameter_tuning import XGBHyperparameterTuner\n",
    "from expected_vaep_model.modelling.optuna_xgb_param_grid import OptunaXGBParamGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tuner = XGBHyperparameterTuner(X_train_concedes, y_train_concedes)\n",
    "xgb_tuner.tune_hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from expected_vaep_model.modelling.supermodel import SuperXGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = xgb_tuner.get_best_params()\n",
    "params['objective'] = OptunaXGBParamGrid.error\n",
    "params['num_rounds'] = 100\n",
    "params['early_stopping_rounds'] = OptunaXGBParamGrid.early_stopping_rounds\n",
    "params['verbosity'] = OptunaXGBParamGrid.verbosity\n",
    "params['monotone_constraints'] = {}\n",
    "\n",
    "print('Fitting model.')\n",
    "super_xgb_concedes = SuperXGBRegressor(X_train = X_train_concedes, \n",
    "                            y_train = y_train_concedes, \n",
    "                            X_test = X_test_concedes, \n",
    "                            y_test = y_test_concedes,\n",
    "                            params = params)\n",
    "super_xgb_concedes.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_concedes_preds = pd.Series(np.clip(super_xgb_concedes.predict(X_train_concedes), 0, 6), name = 'exp_concedes')\n",
    "test_concedes_preds = pd.Series(np.clip(super_xgb_concedes.predict(X_test_concedes), 0, 6), name = 'exp_concedes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_concedes_preds.index = X_train_concedes.index\n",
    "test_concedes_preds.index = X_test_concedes.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from expected_vaep_model.evaluation.model_evaluation import XGBRegressorEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evals_concedes_data = pd.concat([schema_chains.loc[X_train_concedes.index], X_train_concedes, y_train_concedes, train_concedes_preds], axis=1)\n",
    "test_evals_concedes_data = pd.concat([schema_chains.loc[X_test_concedes.index], X_test_concedes, y_test_concedes, test_concedes_preds], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evals_concedes_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evals_concedes_data[['mark_a1', 'mark_a2']] = train_evals_concedes_data[['mark_a1', 'mark_a2']].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concedes_test_evals = XGBRegressorEvaluator(model = super_xgb_concedes.xgb_model,\n",
    "                                       data = test_evals_concedes_data,\n",
    "                                       actual_name = \"exp_concedes_label\",\n",
    "                                       expected_name = \"exp_concedes\"\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evals_concedes_data[['exp_concedes_label', 'exp_concedes']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concedes_test_evals.plot_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concedes_test_evals.plot_feature_importance(max_num_features=10, importance_type=\"total_gain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concedes_test_evals.plot_shap_summary_plot(sample=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected VAEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_chains = pd.concat([schema_chains, gamestate_features, gamestate_labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_chains['exp_scores'] = np.clip(super_xgb_scores.predict(gamestate_features), 0, 6)\n",
    "schema_chains['exp_concedes'] = np.clip(super_xgb_concedes.predict(gamestate_features), 0, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from expected_vaep_model.processing.calculate_exp_vaep import calculate_exp_vaep_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvaep_chains = calculate_exp_vaep_values(schema_chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvaep_chains['Match_ID'] = xvaep_chains['match_id']\n",
    "xvaep_chains['year'] = xvaep_chains['match_id'].apply(lambda x: int(x.split(\"_\")[1]))\n",
    "xvaep_chains['round'] = xvaep_chains['match_id'].apply(lambda x: x.split(\"_\")[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvaep_chains.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_dir = '/Users/ciaran/Documents/Projects/AFL/git-repositories/expected-vaep-model/model_outputs/models'\n",
    "\n",
    "joblib.dump(super_xgb_scores, f'{model_output_dir}/exp_vaep_scores.joblib')\n",
    "joblib.dump(super_xgb_concedes, f'{model_output_dir}/exp_vaep_concedes.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_output_dir = '/Users/ciaran/Documents/Projects/AFL/git-repositories/expected-vaep-model/model_outputs/preprocessors'\n",
    "joblib.dump(xvaep_preproc, f'{preproc_output_dir}/exp_vaep_preprocessor.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload Scored Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AFLPy.AFLData_Client import upload_data, load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_data(Dataset = xvaep_chains[xvaep_chains['year']==2024], Dataset_Name=\"CG_Expected_VAEP\", overwrite=True, update_if_identical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2021, 2025):\n",
    "    # print(year)\n",
    "    upload_data(Dataset = xvaep_chains[xvaep_chains['year']==year], Dataset_Name=\"CG_Expected_VAEP\", overwrite=True, update_if_identical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_data(Dataset = xvaep_chains, Dataset_Name=\"CG_Expected_VAEP\", overwrite=True, update_if_identical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AFLPy.AFLData_Client import metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = metadata(Dataset_Name=\"CG_Expected_VAEP\", ID = \"AFL\")\n",
    "md['Time_Created'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvaep_data = load_data(Dataset_Name=\"CG_Expected_VAEP\", ID = \"AFL\")\n",
    "xvaep_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "expected-vaep-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
